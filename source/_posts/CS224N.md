---
title: CS224N学习笔记
urlname: htqc14mf5
comments: true
mathjax: true
date: 2019-06-02 11:14:00
categories:
- 算法
tags:
- 算法
description: Stanford CS224N NLP学习笔记



---

# Lecture1. Introduction and Word Vectors

## 1. Word2Vec

![Word2Vec中心词预测周围词](/images/CS224N/word2vec_windows.png)

### (1) cbow vs skip-gram

cbow：周围词预测中心词

skip-gram：中心词预测周围词

<https://zhuanlan.zhihu.com/p/37477611>

### (2) Word2Vec原理（skip-gram）

**基本假设**

> Distributional semantics: A word’s meaning is given by the words that frequently appear close-by 

**似然函数Likelihood**: t为词的位置，窗口大小为m

$L(\theta)=\prod_{t=1}^{T}\prod_{j\neq0}^{-m\leq j\leq m}P(w_{t+j}|w_t;\theta)$

**损失函数Objective function**

$J(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{j\neq0}^{-m\leq j\leq m}logP(w_{t+j}|w_t;\theta)$

**单词w的词向量**

$v_w$:当w是中心词center word

$u_w$: 当w是上下文context word

**P的计算：中心词c，上下文单词o，V是词表** (分数显示有点问题用了-1次方)

$P(o|c)={exp(u_o^T v_c)}  ({\sum_{w\in V}exp(u_w^T v_c)})^{-1}$

**梯度下降**

$\frac{\partial}{\partial v_c}log P(o|c) $

$= \frac{\partial}{\partial v_c}u_o^T v_c -  \frac{\partial}{\partial v_c}log(\sum_{w=1}^{V}exp(u_w^T v_c))$

$=u_o-(\sum_{w=1}^{V}exp(u_w^T v_c))^{-1} \sum_{x=1}^{V}\frac{\partial}{\partial v_c}exp(u_x^T v_c)$

$=u_o-(\sum_{w=1}^{V}exp(u_w^T v_c))^{-1} \sum_{x=1}^{V}exp(u_x^T v_c)u_x$

$=u_o-\sum_{x=1}^{V} exp(u_x^T v_c) (\sum_{w=1}^{V}exp(u_w^T v_c))^{-1} u_x$

$=u_o-\sum_{x=1}^{V}P(x|c)u_x$

