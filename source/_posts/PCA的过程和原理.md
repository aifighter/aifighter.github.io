---
title: PCA的过程和原理
urlname: y4wbsi1d
comments: true
mathjax: true
date: 2019-06-10 18:31:00
categories:
- 算法
tags:
- 预处理
description: PCA是一种无参的降维技术，理解其中的过程以及背后的原理
---

# 1. PCA的步骤

假设有m条n维的数据

1. 构建矩阵$X_{n\times m}$
2. 将$X$的每一行减去对应的均值
3. 求出$X$的协方差矩阵$C=\frac{1}{m}XX^{\top}$
4. 求出$C$的特征值和特征向量
5. 将特征向量按特征值从大到小从上至下排列组成矩阵，取前$k$行组成矩阵$P_{k\times n}$
6. $Y=PX$即为PCA后的结果

# 2. PCA的原理

### （1）内积与投影

$A\cdot B=|A||B|\cos\alpha$：$A$与$B$的点积等于$A$在$B$上的投影乘以$B$的模

若$B$是单位向量，那么$A\cdot B=|A|\cos\alpha$

### （2）基变换的矩阵表示

$A_{2\times2}\vec x=\vec y$

相当于将向量$\vec x$变换到以$A$的行向量为新的标准基的坐标，因为$A$的行向量与$\vec x$的点积相当于在新的基上的投影，即为在该基上的坐标。

$A_{k\times n}B_{n\times m}=C_{r\times m}$

上面的操作将m个n维向量映射到一个r维的基中，k小于n就实现了降维

### （3）从一维开始

若要将数据降维成一维，那么需要寻找一个基来做坐标变换，并且所有数据转换到这个基上后，方差最大

### （4）提升到二维

第二个维度的基需要满足：（1）与第一个基线性无关（2）数据在该基上的投影方差最大

### （5）协方差

数学上$cov(X,Y)=E(X-\mu)(Y-\nu)$，若协方差为0则两个变量相互独立

对于PCA中的字段，由于已经做了减均值的操作，因此$cov(a,b)=\frac{1}{m}\sum_{i=1}^m{a_i b_i}$

### （6）PCA的目标

将一组N维向量映射到K维，其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0（即每一个特征维度都是线性无关的），而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 

### （7）协方差矩阵

$X=[X_1, X_2, ..., X_n]^\top$：$X$是以n个变量组成的列向量，$\mu_i$是$X_i$的期望

协方差矩阵为：

$\sum=E[(X-E(X))(X-E(X))^\top]$

其中：

$\sum_{ij}=cov(X_i, X_j)=E[(X_i-\mu_i)(X_j-\mu_j)]$

### 结论

假设PCA前的原始矩阵为$X_{n\times m}$，变换矩阵为$P_{k\times n}$，变换后的矩阵为$Y_{k\times m}=PX$

假设$X$的协方差矩阵为$C=\frac{1}{m}XX^\top$，$Y$的协方差矩阵为$D=\frac{1}{m}YY^\top$(因为均值都是0)

那么就有

$D=\frac{1}{m}YY^\top$

$=\frac{1}{m}(PX)(PX)^\top$

$=\frac{1}{m}PXX^\top P^\top$

$=\frac{1}{m}P(XX^\top) P^\top$

$=\frac{1}{m}PCP^\top$

我们的目的是要让$D$是一个对角矩阵（满足各个特征线性无关），且对角元素越大越好（方差最大）

所以我们要找的$P$就是能够使$C$对角化的矩阵，这个在数学上很简单

$P$就是把$C$的特征向量按大小在行上从大到小排列的矩阵！

> 参考资料：<https://zhuanlan.zhihu.com/p/21580949>

