---
title: SVM
urlname: 3sqc2yre
comments: true
mathjax: true
date: 2020-04-11 17:19:00
categories:
- 算法
tags:
- 算法
description: 介绍支持向量机的原理
---

### 0. 问题

给定数据集$\mathcal{D}=\{(\vec{x_1}, y_1),(\vec{x_2}, y_2)...(\vec{x_N}, y_N)\}$，其中y用+1表示正例，-1表示负例，学习模型$f(\vec{x})$

### 1. 基本思想

SVM就是要在样本之间划分一个超平面，并且样本到超平面的最小距离要足够大。其中距离超平面最近的那些样本就是支撑点，所以叫支持向量机。

非支撑点对SVM的模型最终结果没有任何影响。

### 2. 线性可分支持向量机

#### （1）假设

这是SVM最基础的版本，假设样本点可以由一个超平面分开

#### （2）模型

$$
f(\vec{x})=sign(\vec{w}^*\cdot\vec{x}+b^*)
$$

#### （3）求解过程

最优化下式得到参数$\vec{w}^*$和$\vec{b}^*$
$$
min\frac{1}{2}||\vec{w}||^2_2 \\
s.t. \tilde{y_i}(\vec{w}\cdot\vec{x}_i+b)-1\geq{0},i=1,2,..,N
$$

#### （4）解释

- 对于一个超平面$\vec{w}\cdot\vec{x}+\vec{b}=0$，一个点$\vec{x}_i$到该平面的距离正比于$\vec{w}\cdot\vec{x}_i+b$，为正则为上方，负是下方。
- 上面的限制条件中乘以$\tilde{y}_i$就确保了正样本都在上方，负样本都在下方。

为了让点到平面的距离最远，等价于最大化满足下式的$\hat{\gamma}$
$$
\tilde{y_i}(\vec{w}\cdot\vec{x}_i+b)\ge\hat{\gamma}
$$
由于$\vec{w}$和$b$同时乘以一个100平面不变，但是$\hat{\gamma}$变大，因此实际上需要最大化满足下式的$\gamma$
$$
\tilde{y_i}(\frac{\vec{w}}{||\vec{w}||^2_2}\cdot\vec{x}_i+\frac{b}{||\vec{w}||^2_2})\ge\gamma
$$
其中
$$
\gamma=\frac{\hat{\gamma}}{{||\vec{w}||^2_2}}
$$
又因为$\hat{\gamma}$是可以任取的，我们规定它为1，那么上面的最优化就变成了（3）中描述的问题

#### （5）最优化方法

称为对偶算法，不展开

### 2. 线性不可分支持向量机

部分样本很难被线性可分，比如一个样本的标签错了，或者有几个异常样本会导致SVM效果很差，我们需要忽略部分样本的影响，这个时候可以加入一个松弛变量$\xi_i$，最优化问题变为
$$
min\frac{1}{2}||\vec{w}||^2_2+C\sum{\xi_i}\\
s.t. \tilde{y_i}(\vec{w}\cdot\vec{x}_i+b)-1\geq{-\xi_i},i=1,2,..,N\\
\xi_i\ge0,i=1,2,..,N
$$


C是用来调节对异常样本的容忍程度，0为完全不能容忍，线性不可分问题将无解

### 3. 非线性支持向量机

利用种种核函数将原问题转换到新的空间中再进行上面的方法

### 4. SVDD

用于一分类（即是或不是，样本只给一类），学习一个最小半径的球面将所有样本包裹进去。

### 5. SVM优缺点

优点：

1. 可解释性强
2. 可以找到关键样本（支撑点）

缺点：

1. 优化困难
2. 时间空间复杂度高



