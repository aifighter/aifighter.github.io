---
title: 交叉熵相对熵与KL散度
urlname: sa6exzdb
comments: true
mathjax: true
date: 2019-06-21 16:36:00
categories:
- 数学
tags:
- 信息论
description: 如何理解交叉熵与相对熵？什么是KL散度？交叉熵损失函数是什么？

---

# 1. 定义

一个事件的自信息为：

$I(x)=-logP(x)$

一个随机变量X，它的信息熵为：

$H(X)=E(I(X))=-\sum{P(X)log(P(X))}$

对两个随机变量P和Q，概率分布分布为p和q，它们的交叉熵为：

$H(P, Q)=-\sum{p(x_i)log(q(x_i))}$

相对熵，也称为KL散度，用来衡量两个概率分布的差异：

$D_{KL}(P||Q)=H(P,Q)-H(P)=\sum{p(x_i)log\frac{p(x_i)}{q(x_i)}}$

# 2. 理解

### 理解1

假设p为真实分布，q是为了逼近p的非真实分布，那么KL散度反映的就是两个分布的差异，当q和p一致时，KL散度就变成了0，那么q完美地逼近了p。这也是机器学习的交叉熵损失函数的由来，交叉熵越小则p，q约接近。p可以看成是真实的“完美”模型，那么q就是我们训练出来的模型。

### 理解2

在信息论的编码中可以这样理解：

- 信息熵：编码方案完美时，最短平均编码长度的是多少

- 交叉熵：编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度的是多少

- 相对熵：编码方案不一定完美时，平均编码长度相对于最小值的增加值

> 参考资料：[如何通俗的解释交叉熵与相对熵?](https://www.zhihu.com/question/41252833)

