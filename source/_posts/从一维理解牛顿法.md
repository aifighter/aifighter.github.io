---
title: 从一维理解牛顿法
urlname: f57kwm75
comments: true
mathjax: true
date: 2020-04-03 17:46:00
categories:
- 数学
tags:
- 数值分析
description: 做优化问题时最常用的就是梯度下降和牛顿法了，那么牛顿法到底是怎么回事？

---

# 1. 牛顿法用于求根

牛顿法最初用于解方程，比如求解$y=f(x)$的根

它的思想是这样的：

1. 找一个初始值$x_0$
2. 过$x_0$做一条切线，切线与x轴交于$x_1$
3. 重复上述过程n次，得到的$x_n$即为近似解

非常容易得到：
$$
x_{n+1}=x_n-\frac{f(x_n)}{f'(x)}
$$
OK，这个就是牛顿法了

# 2. 牛顿法用于最优化

最优化就是求$y=f(x)$的极值，一般为最小值

那么我们知道，极值点满足$f'(x)=0$，那么也就是相当于求根

那么牛顿法做优化问题的迭代就是
$$
x_{n+1}=x_n-\frac{f'(x_n)}{f''(x_n)}
$$
这样理解就方便多了

# 3. 回顾梯度下降

梯度下降同样用于求$y=f(x)$的极值，一般为最小值

它的做法是：
$$
x_{n+1}=x_n-\epsilon{f'(x_n)}
$$
和牛顿法的差别在于，一个使用了一阶导，另一个使用了二阶导

牛顿法理论上要快很多

# 4. 推广到高维

梯度下降推广到高维特别容易，上面的式子不用做任何改变

牛顿法则很困难，因为涉及到了二阶导，需要借助于海森矩阵。在深度学习中，维度动辄百万千万，因此不适合使用牛顿法。