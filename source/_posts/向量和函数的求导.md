---
title: 向量和函数的求导
urlname: 6h5fxyiy
comments: true
mathjax: true
date: 2020-03-31 00:25:00
categories:
- 数学
tags:
- 线性代数
description: 经常会搞不清向量或矩阵求导到底求的是啥
---

# 1. 向量对标量求导

> 向量中的每个元素对标量求导，结果是一个向量

$$
\frac{\partial\vec y}{\partial x}=(\frac{\partial y_1}{\partial x}, …)^T
$$

# 2. 矩阵对向量求导

> 矩阵中的每个元素对标量求导，结果是一个矩阵，类似向量对标量求导

# 3. 标量对向量求导

> 标量对向量中的每个元素求导，结果是一个向量

这里的标量f一般是x的函数

$$
\frac{\partial f}{\partial{\vec{x}}}=(\frac{\partial f}{\partial x_1}, …)^T
$$

实际上这就是梯度下降中的gradient

# 4. 向量对向量求导

> 向量对向量求导，结果是一个矩阵，这个矩阵也叫Jacobi矩阵

$$
\vec f=(f_1,f_2,...,f_m)^T \\
\vec x=(x_1,x_2,...,x_n)^T \\
\nabla f=\frac{\partial \vec f^T}{\partial \vec x}
=[\frac{\partial f_1}{\partial \vec x},\frac{\partial f_2}{\partial \vec x},...,\frac{\partial f_m}{\partial \vec x}]=\left[
\begin{matrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} & ... \\
\frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} & ... \\
... & ... & ...
\end{matrix}
\right]
$$

实际上就是当一个网络有两个损失函数的时候需要计算的一个东西