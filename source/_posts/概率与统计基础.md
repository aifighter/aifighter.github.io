---
title: 概率与统计基础
urlname: n3t7eh59
comments: true
mathjax: true
date: 2019-06-14 12:18:00
categories:
- 数学
tags:
- 概率与统计
description: 深度学习基础之概率论与数理统计
---

# 1. 如何理解贝叶斯定理

$P(A|B)=P(A)\frac{P(B|A)}{P(B)}$

新信息出现后A的概率 = A的概率 * 新信息带来的调整

例子：

- A：前方出现十字路口
- B：前车打了右转向灯

本身车开在路上，前方出现十字路口的概率不高，但是前车打了右转向灯，那么前方是十字路口的概率就大大提高了。

# 2. 如何理解泊松分布

以下情况适用泊松分布：

一家店平均每天会来5个人，有时多，有时少，那么想要知道每天来店的人数的分布。

具体做法如下：

把一天的时间分成$n$份$n \to \infty$，那么每一份时间最多来一个客人，且每一个时间来人是独立的，假设平均每一天来$\lambda$人，且每一份时间来人的概率就是$p=\frac{\lambda}{n}$，那么一天中来人的数量为$k$的概率就是一个二项分布了：

$P(X=k)=\lim_{n \to\infty}C_n^k(\frac{\lambda}{n})^k {(1-\frac{\lambda}{n})}^{n-k}=\frac{\lambda^{k}}{k!}e^{-\lambda}$

在现实生活中，类似排队这样的场景都可以用泊松分布来描述

# 3. 如何理解指数分布

> 指数分布就是泊松分布的时间间隔

首先对泊松分布做一个扩展，上面的泊松分布只能描述“一天”内的情况，那么“半天”怎么描述呢

$P(X=k,t)=\frac{(\lambda{t})^{k}}{k!}e^{-\lambda{t}}$

那么“两次来客的间隔大于t的概率”=”t时间内没有客人来的概率“

$P(Y>t)=P(X=0,t)=e^{-\lambda t}$

对应的：

$P(Y\leq y)=1-e^{-\lambda y}, y \geq 0$

概率密度函数：

$p(y)=\lambda e^{-\lambda y}, y \geq 0$

泊松分布和指数分布的期望：

$E(X)=\lambda$，$E(Y)=\frac{1}{\lambda}$

# 4. 中心极限定理

中心极限定理指的是给定一个任意分布的总体。我每次从这些总体中随机抽取 n 个抽样，一共抽 m 次。 然后把这 m 组抽样分别求出平均值。 这些平均值的分布接近正态分布。

很多独立随机变量的和近似服从正态分布。

# 5. 大数定律

通俗地讲，样本足够大时，样本均值趋向于整体期望

# 6. 正态分布

概率密度函数，其中$\mu$是均值，$\sigma^2$是方差

$f(x)=\frac{1}{\sigma\sqrt{2\pi}}exp(\frac{-(x-\mu)^2}{2\sigma^2})$

# 7. 如何理解概率论中的矩

矩和物理中的力矩很像，$p(x)$就像是力矩，$x$就像是力。

一阶矩是期望，二阶矩是方差。

一阶矩：$\mu_1^{'}=\int{xp(x)dx}=E(x)$

n阶矩：$\mu_n^{'}=\int{(x-E(x))^{n}p(x)dx}$

# 8. 为什么样本方差的分母是n-1

假设随机变量$X$，期望是$\mu=E(X)$，方差$\sigma^2=E[(X-\mu)^2]$。

现在采样$n$个值来对$X$做估计

1. 用$\overline{X}=\frac{1}{n}\sum X_i$来估计$\mu$是一个无偏估计
2. 用$S^2=\frac{1}{n}\sum{(X_i-\mu)^2}$来估计$\sigma^2$也是一个无偏估计（然而$\mu$是不知道的）
3. 用$\frac{1}{n}\sum{(X_i-\overline{X})^2}$来估计$\sigma^2$是一个有偏估计
4. 用$S^2=\frac{1}{n-1}\sum{(X_i-\overline{X})^2}$来估计$\sigma^2$才是一个无偏估计

定性来看3为什么是有偏的，因为$\overline{X}$是不准的，所以会导致$\frac{1}{n}\sum{(X_i-\overline{X})^2}$偏小（想象一下0和2两个数，一个用1算方差，一个用0），所以要调整一下让它正常。经过计算4是无偏的。

# 9. 最大似然估计

用实际发生的情况，去找到使这个情况发生概率最大的参数，就是最大似然估计。

假设做了n次实验，或者说n次采样，那么似然函数为：

$L(\theta|x_1,x_2,...,x_n)=f_\theta(x_1,x_2,...,x_n)$

其中$f_\theta$是观察到这n个结果的概率，那么使得$f_\theta$最大的$\theta$就是最大似然估计得到的参数

特别地，当n次实验互相独立的时候：

$L(\theta|x_1,x_2,...,x_n)=f(x_1|\theta)f(x_2|\theta)...f(x_n|\theta)$

# 10. 如何理解协方差和相关系数

协方差反映的是两个变量之间的相关性

$cov(X,Y)=E[(x-E(X))(y-E(Y))]$

样本协方差等于$S_{XY}=\frac{1}{n-1}\sum{(X_i-\overline{X})(Y_i-\overline{Y})}$

相关系数是将协方差归一后的结果，更方便做比较

$\eta=\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}$

样本的相关系数，则是将分子替换为样本协方差，分母替换为样本方差

# 11. 如何理解置信区间

置信区间是用样本测量值来估计真实值所在的区间，真实值在这个区间的概率称为置信度

假设人的身高符合$X\sim N(\mu,\sigma^2)$

我们抽样$n$个人，它们的平均身高为$M$，根据大数定律和中心极限定理，$M\sim N(\mu, \frac{\sigma^2}{n})$

那么随机抽的这个$M$，计算占这个分布中心95%的面积的区间

$P(\mu-1.96\frac{\sigma}{\sqrt{n}}\leq M \leq \mu+1.96\frac{\sigma}{\sqrt{n}})=0.95$

那么真实的$\mu$符合

$P(M-1.96\frac{\sigma}{\sqrt{n}}\leq \mu \leq M+1.96\frac{\sigma}{\sqrt{n}})=0.95$

# 12. 如何理解假设检验和P值

假设检验就是说我先假设一个结论，然后根据实验结果，把所有比这个实验结果更为“极端”的情况加起来，算一个概率称为P值，如果P值非常小，就可以认为假设是错误的。

举个例子，抛一枚硬币10次

假设：硬币两边等概

实验结果：抛了10次9次正面朝上

P值：$P(X\geq 9)=0.01$（单边P值），$P(X\geq 9)+P(X\leq 1)=0.02$（双边P值）

显著水平：一般认为显著水平为0.05，那么P值小于0.05就可以驳回假设了。

# 13. 如何理解最小二乘法

最小二乘法就是让误差的平方和最小

$\epsilon=\sum(y-y_i)^2$

假设$y=ax+b$，那么就可以代入$\epsilon$然后用导数为0的方法求得对应的$a$和$b$



