---
title: 集成学习
urlname: aa7ow6og
comments: true
mathjax: true
date: 2020-04-29 11:46:00
categories:
- 算法
tags:
- 算法
description: 集成学习主要包括boosting和bagging，有助于将多个弱分类器变成强分类器


---

# 1. Boosting

#### 1.1 特点

1. 按顺序学习分类器，根据上一个分类器的效果来决定下一个分类器
2. 上一个分类器中学得好的样本权重降低，反之权重上升
3. 优秀的分类器权重上升，反之下降

#### 1.2 Adaboost

步骤

1. 初始化训练数据，每个训练数据的权重一致，并训练第一个分类器$M_1$

2. 循环：

   （1）根据第$m$个分类器的分类效果决定该分类器的权重$\alpha_{m}$

   （2）根据第$m$个分类器训练数据的权重分布$W_m$以及分类效果决定$W_{m+1}$

3. 最终得到的分类器为：

   $$H(\vec{x})=sign(\sum_{m=1}^M{\alpha_m{h_m(\vec{x})}})$$

Adaboost还可以推广到多分类以及回归中

# 2. Bagging

#### 2.1 特点

Bagging基于bootstrap sampling，即有放回的随机取样

#### 2.2 步骤

1. M次bootstrap sampling得到M组训练数据
2. 训练M个学习器
3. 综合M个学习器得到最终的学习器

#### 2.3 随机森林

随机森林在Bagging的基础上加入了属性的选择，即每一颗树只使用部分属性，数量一般是$log_2{n}$